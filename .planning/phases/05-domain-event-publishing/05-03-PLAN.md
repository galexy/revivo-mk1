---
phase: 05-domain-event-publishing
plan: 03
type: execute
wave: 2
depends_on: ["05-02"]
files_modified:
  - scripts/jobs/list-failed.sh
  - scripts/jobs/retry-job.sh
  - scripts/jobs/retry-all-failed.sh
  - scripts/jobs/job-status.sh
  - scripts/jobs/queue-stats.sh
  - scripts/jobs/purge-old.sh
  - scripts/jobs/apply-schema.sh
  - docs/runbooks/job-queue.md
autonomous: true

must_haves:
  truths:
    - "Operators can list failed jobs from command line"
    - "Operators can retry failed jobs individually or in bulk"
    - "Operators can view job queue statistics"
    - "Runbook documents common operational procedures"
    - "Scripts use Procrastinate CLI or direct SQL queries"
  artifacts:
    - path: "scripts/jobs/list-failed.sh"
      provides: "List failed jobs with error details"
    - path: "scripts/jobs/queue-stats.sh"
      provides: "Job counts by status"
    - path: "docs/runbooks/job-queue.md"
      provides: "Operational runbook for job queue"
      min_lines: 100
  key_links:
    - from: "scripts/jobs/*.sh"
      to: "src/adapters/jobs/app.py"
      via: "procrastinate CLI with app path"
      pattern: "procrastinate.*--app.*src.adapters.jobs.app"
---

<objective>
Create operational tooling for job queue management since Procrastinate doesn't have a Django admin dashboard for FastAPI apps.

Purpose: Give operators visibility into job queue health, ability to debug failed jobs, and documented procedures for common tasks. This is essential for production operations.

Output:
- Shell scripts for common job queue operations
- Markdown runbook with procedures and troubleshooting
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05-domain-event-publishing/05-CONTEXT.md
@.planning/phases/05-domain-event-publishing/05-RESEARCH.md
@src/adapters/jobs/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job queue operational scripts</name>
  <files>
    - scripts/jobs/list-failed.sh
    - scripts/jobs/retry-job.sh
    - scripts/jobs/retry-all-failed.sh
    - scripts/jobs/job-status.sh
    - scripts/jobs/queue-stats.sh
    - scripts/jobs/purge-old.sh
    - scripts/jobs/apply-schema.sh
  </files>
  <action>
1. Create `scripts/jobs/` directory.

2. Create `scripts/jobs/apply-schema.sh`:
```bash
#!/usr/bin/env bash
# Apply Procrastinate schema to jobs database
# Run this before starting workers for the first time
set -euo pipefail

echo "Applying Procrastinate schema to jobs database..."
procrastinate --app=src.adapters.jobs.app.job_queue schema --apply
echo "Schema applied successfully."
```

3. Create `scripts/jobs/list-failed.sh`:
```bash
#!/usr/bin/env bash
# List failed jobs with error information
# Usage: ./list-failed.sh [queue_name] [limit]
set -euo pipefail

QUEUE="${1:-}"
LIMIT="${2:-20}"

# Use procrastinate shell to query failed jobs
# Falls back to direct SQL if shell doesn't support filtering
cat << 'SQL' | psql "$JOB_QUEUE_DATABASE_URL" -t
SELECT
    id,
    queue_name,
    task_name,
    created_at,
    attempts,
    scheduled_at
FROM procrastinate_jobs
WHERE status = 'failed'
ORDER BY created_at DESC
LIMIT ${LIMIT};
SQL

echo ""
echo "To see error details for a job: ./job-status.sh <job_id>"
```

4. Create `scripts/jobs/job-status.sh`:
```bash
#!/usr/bin/env bash
# Get detailed status of a specific job
# Usage: ./job-status.sh <job_id>
set -euo pipefail

JOB_ID="${1:?Usage: ./job-status.sh <job_id>}"

cat << SQL | psql "$JOB_QUEUE_DATABASE_URL" -x
SELECT
    j.id,
    j.queue_name,
    j.task_name,
    j.status,
    j.args,
    j.created_at,
    j.scheduled_at,
    j.started_at,
    j.attempts,
    e.error,
    e.error_traceback
FROM procrastinate_jobs j
LEFT JOIN procrastinate_events e ON j.id = e.job_id AND e.type = 'error'
WHERE j.id = ${JOB_ID}
ORDER BY e.at DESC
LIMIT 1;
SQL
```

5. Create `scripts/jobs/retry-job.sh`:
```bash
#!/usr/bin/env bash
# Retry a specific failed job
# Usage: ./retry-job.sh <job_id>
set -euo pipefail

JOB_ID="${1:?Usage: ./retry-job.sh <job_id>}"

echo "Retrying job ${JOB_ID}..."

cat << SQL | psql "$JOB_QUEUE_DATABASE_URL"
UPDATE procrastinate_jobs
SET status = 'todo', scheduled_at = NOW()
WHERE id = ${JOB_ID} AND status = 'failed';
SQL

echo "Job ${JOB_ID} queued for retry."
```

6. Create `scripts/jobs/retry-all-failed.sh`:
```bash
#!/usr/bin/env bash
# Retry all failed jobs in a queue
# Usage: ./retry-all-failed.sh [queue_name]
set -euo pipefail

QUEUE="${1:-}"

if [ -z "$QUEUE" ]; then
    echo "Retrying ALL failed jobs..."
    QUERY="UPDATE procrastinate_jobs SET status = 'todo', scheduled_at = NOW() WHERE status = 'failed';"
else
    echo "Retrying failed jobs in queue: ${QUEUE}..."
    QUERY="UPDATE procrastinate_jobs SET status = 'todo', scheduled_at = NOW() WHERE status = 'failed' AND queue_name = '${QUEUE}';"
fi

RESULT=$(echo "$QUERY" | psql "$JOB_QUEUE_DATABASE_URL" -t)
echo "$RESULT"
```

7. Create `scripts/jobs/queue-stats.sh`:
```bash
#!/usr/bin/env bash
# Show job queue statistics
# Usage: ./queue-stats.sh
set -euo pipefail

echo "=== Job Queue Statistics ==="
echo ""

cat << 'SQL' | psql "$JOB_QUEUE_DATABASE_URL" -t
SELECT
    queue_name,
    status,
    COUNT(*) as count
FROM procrastinate_jobs
GROUP BY queue_name, status
ORDER BY queue_name, status;
SQL

echo ""
echo "=== Recent Failures (last 24h) ==="

cat << 'SQL' | psql "$JOB_QUEUE_DATABASE_URL" -t
SELECT
    task_name,
    COUNT(*) as failures
FROM procrastinate_jobs
WHERE status = 'failed'
  AND created_at > NOW() - INTERVAL '24 hours'
GROUP BY task_name
ORDER BY failures DESC;
SQL
```

8. Create `scripts/jobs/purge-old.sh`:
```bash
#!/usr/bin/env bash
# Purge completed jobs older than N days
# Usage: ./purge-old.sh <days> [--include-failed]
set -euo pipefail

DAYS="${1:?Usage: ./purge-old.sh <days> [--include-failed]}"
INCLUDE_FAILED="${2:-}"

echo "Purging jobs older than ${DAYS} days..."

if [ "$INCLUDE_FAILED" = "--include-failed" ]; then
    echo "Including failed jobs in purge."
    QUERY="DELETE FROM procrastinate_jobs WHERE (status = 'succeeded' OR status = 'failed') AND created_at < NOW() - INTERVAL '${DAYS} days';"
else
    echo "Only purging succeeded jobs (use --include-failed to also purge failed)."
    QUERY="DELETE FROM procrastinate_jobs WHERE status = 'succeeded' AND created_at < NOW() - INTERVAL '${DAYS} days';"
fi

RESULT=$(echo "$QUERY" | psql "$JOB_QUEUE_DATABASE_URL" -t)
echo "$RESULT"
```

9. Make all scripts executable:
```bash
chmod +x scripts/jobs/*.sh
```
  </action>
  <verify>
    - All scripts exist in scripts/jobs/
    - All scripts are executable: `ls -la scripts/jobs/*.sh`
    - Scripts have valid bash syntax: `bash -n scripts/jobs/*.sh`
  </verify>
  <done>
    - 7 operational scripts created
    - All scripts executable
    - Scripts use JOB_QUEUE_DATABASE_URL for connection
  </done>
</task>

<task type="auto">
  <name>Task 2: Create job queue runbook</name>
  <files>
    - docs/runbooks/job-queue.md
  </files>
  <action>
1. Create `docs/runbooks/` directory if needed.

2. Create `docs/runbooks/job-queue.md` with comprehensive operational documentation:

```markdown
# Job Queue Operations Runbook

This runbook covers operational procedures for the Procrastinate-based job queue.

## Overview

The job queue handles async side effects (email sending, external API calls) triggered by domain events. It uses:
- **Procrastinate** - PostgreSQL-based job queue
- **Separate database** - `postgres-jobs` service, not the main app database
- **In-process worker** - runs alongside FastAPI via lifespan

## Quick Reference

| Task | Command |
|------|---------|
| View queue stats | `./scripts/jobs/queue-stats.sh` |
| List failed jobs | `./scripts/jobs/list-failed.sh` |
| Get job details | `./scripts/jobs/job-status.sh <id>` |
| Retry single job | `./scripts/jobs/retry-job.sh <id>` |
| Retry all failed | `./scripts/jobs/retry-all-failed.sh [queue]` |
| Purge old jobs | `./scripts/jobs/purge-old.sh <days>` |
| Apply schema | `./scripts/jobs/apply-schema.sh` |

## Environment Setup

Scripts require `JOB_QUEUE_DATABASE_URL` environment variable:

```bash
export JOB_QUEUE_DATABASE_URL="postgresql://postgres:postgres@localhost:5433/jobs"
```

In docker-compose, this is set automatically for the app service.

## Common Procedures

### 1. Check Queue Health

Run daily or when investigating issues:

```bash
./scripts/jobs/queue-stats.sh
```

**Healthy indicators:**
- No jobs stuck in `doing` status for >5 minutes
- Failed job count stable (not growing)
- Succeeded jobs being processed

**Warning signs:**
- Growing backlog of `todo` jobs
- Jobs stuck in `doing` (worker may have crashed)
- Spike in failed jobs

### 2. Investigate Failed Jobs

When you see failed jobs:

```bash
# List recent failures
./scripts/jobs/list-failed.sh

# Get details on a specific job
./scripts/jobs/job-status.sh 123
```

**Common failure patterns:**

| Error | Likely Cause | Fix |
|-------|--------------|-----|
| `ConnectionError` | External service down | Wait and retry |
| `TimeoutError` | Service slow/overloaded | Retry with backoff |
| `ValidationError` | Bad job arguments | Check event payload, may need code fix |
| `DatabaseError` | DB connection issue | Check postgres-jobs health |

### 3. Retry Failed Jobs

After fixing the root cause:

```bash
# Retry a single job
./scripts/jobs/retry-job.sh 123

# Retry all failed jobs in a queue
./scripts/jobs/retry-all-failed.sh email

# Retry ALL failed jobs (use carefully)
./scripts/jobs/retry-all-failed.sh
```

### 4. Manual Job Inspection

For deep debugging, use Procrastinate shell:

```bash
procrastinate --app=src.adapters.jobs.app.job_queue shell
```

Or connect directly to the jobs database:

```bash
psql $JOB_QUEUE_DATABASE_URL
```

Useful queries:

```sql
-- Jobs stuck in 'doing' for >5 minutes (potentially stalled)
SELECT * FROM procrastinate_jobs
WHERE status = 'doing'
  AND started_at < NOW() - INTERVAL '5 minutes';

-- Recent errors by type
SELECT
    task_name,
    error,
    COUNT(*)
FROM procrastinate_jobs j
JOIN procrastinate_events e ON j.id = e.job_id
WHERE e.type = 'error'
  AND e.at > NOW() - INTERVAL '1 hour'
GROUP BY task_name, error;
```

### 5. Maintenance: Purge Old Jobs

Run weekly or monthly to prevent table bloat:

```bash
# Purge succeeded jobs older than 30 days
./scripts/jobs/purge-old.sh 30

# Also purge old failed jobs (after investigation)
./scripts/jobs/purge-old.sh 90 --include-failed
```

**Recommendation:**
- Keep succeeded jobs: 30 days
- Keep failed jobs: 90 days (for debugging patterns)

## Troubleshooting

### Worker Not Processing Jobs

**Symptoms:** Jobs stuck in `todo`, no progress

**Checks:**
1. Is the API service running? (worker runs in same process)
2. Check logs for worker startup: `"job_queue_worker_started"`
3. Check for connection errors to postgres-jobs

**Fix:**
- Restart the API service
- Verify `JOB_QUEUE_DATABASE_URL` is correct
- Check postgres-jobs is healthy: `docker compose ps postgres-jobs`

### Jobs Failing Repeatedly

**Symptoms:** Same job fails, retries, fails again

**Checks:**
1. Get job details: `./scripts/jobs/job-status.sh <id>`
2. Check error_traceback for root cause
3. Check if external service is down

**Fix:**
- If transient: wait for retries to succeed
- If permanent: fix code, then retry job
- If bad data: may need to manually mark as succeeded or delete

### Database Connection Issues

**Symptoms:** `could not connect to server` errors

**Checks:**
1. Is postgres-jobs running? `docker compose ps postgres-jobs`
2. Is the port correct? (5433 for jobs DB, 5432 for main DB)
3. Network connectivity between containers

**Fix:**
```bash
# Restart jobs database
docker compose restart postgres-jobs

# Check logs
docker compose logs postgres-jobs
```

### Schema Not Applied

**Symptoms:** `relation "procrastinate_jobs" does not exist`

**Fix:**
```bash
./scripts/jobs/apply-schema.sh
```

This should be run once when setting up a new environment.

## Monitoring Queries

For integration with monitoring systems (Grafana, etc.):

```sql
-- Failed jobs in last hour (for alerting)
SELECT COUNT(*) FROM procrastinate_jobs
WHERE status = 'failed'
  AND created_at > NOW() - INTERVAL '1 hour';

-- Queue depth by queue (for dashboards)
SELECT queue_name, COUNT(*) as pending
FROM procrastinate_jobs
WHERE status = 'todo'
GROUP BY queue_name;

-- Processing rate (jobs/hour)
SELECT COUNT(*) as processed_last_hour
FROM procrastinate_jobs
WHERE status = 'succeeded'
  AND created_at > NOW() - INTERVAL '1 hour';
```

## Emergency Procedures

### Stop All Job Processing

If jobs are causing problems and need to stop immediately:

```bash
# Restart API without worker (temporary)
# Set environment variable to skip worker startup
SKIP_JOB_WORKER=true uvicorn src.adapters.api.app:app
```

Or stop the API service entirely.

### Clear All Pending Jobs

**Use with extreme caution** - jobs will be lost:

```sql
DELETE FROM procrastinate_jobs WHERE status = 'todo';
```

### Reset a Stuck Job

If a job is stuck in `doing` (worker crashed mid-processing):

```sql
UPDATE procrastinate_jobs
SET status = 'todo', started_at = NULL
WHERE id = <job_id> AND status = 'doing';
```

## Architecture Notes

- **Event bus** dispatches domain events synchronously after UoW commit
- **Event handlers** enqueue jobs for async processing
- **Jobs database** is separate from main app database (isolation)
- **Retries** use exponential backoff (60s, 3600s, 216000s...)
- **Failed jobs** stay in database for debugging (not auto-deleted)

## Related Documentation

- [Procrastinate Docs](https://procrastinate.readthedocs.io/)
- Phase 5 Research: `.planning/phases/05-domain-event-publishing/05-RESEARCH.md`
- Phase 5 Context: `.planning/phases/05-domain-event-publishing/05-CONTEXT.md`
```
  </action>
  <verify>
    - Runbook exists: `cat docs/runbooks/job-queue.md | head -20`
    - Runbook has required sections: Quick Reference, Common Procedures, Troubleshooting
    - Runbook is >100 lines: `wc -l docs/runbooks/job-queue.md`
  </verify>
  <done>
    - Comprehensive runbook created
    - Covers health checks, debugging, retry procedures
    - Includes monitoring queries for alerting
    - Documents emergency procedures
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Scripts exist and are executable:
   ```bash
   ls -la scripts/jobs/*.sh
   ```

2. Scripts have valid syntax:
   ```bash
   for f in scripts/jobs/*.sh; do bash -n "$f" && echo "$f OK"; done
   ```

3. Runbook exists with key sections:
   ```bash
   grep -E "^## " docs/runbooks/job-queue.md
   ```

4. All existing tests still pass:
   ```bash
   pytest tests/ -x -q
   ```
</verification>

<success_criteria>
1. 7 operational scripts in scripts/jobs/
2. All scripts executable and valid bash
3. Runbook >100 lines with procedures and troubleshooting
4. Scripts use JOB_QUEUE_DATABASE_URL for database connection
5. All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-domain-event-publishing/05-03-SUMMARY.md`
</output>
