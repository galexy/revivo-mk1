---
phase: 05-domain-event-publishing
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/adapters/jobs/__init__.py
  - src/adapters/jobs/app.py
  - src/adapters/jobs/tasks.py
  - src/adapters/persistence/unit_of_work.py
  - src/adapters/api/app.py
  - src/application/handlers/__init__.py
  - src/application/handlers/user_handlers.py
  - tests/integration/test_event_publishing.py
autonomous: true

must_haves:
  truths:
    - "Events are dispatched to handlers AFTER UoW commit succeeds"
    - "Event handlers can enqueue jobs to Procrastinate"
    - "Worker starts alongside FastAPI in lifespan"
    - "Handler registration happens at application startup"
    - "UserRegistered event triggers a handler"
  artifacts:
    - path: "src/adapters/jobs/app.py"
      provides: "Procrastinate job queue app instance"
      exports: ["job_queue", "create_job_queue"]
    - path: "src/adapters/jobs/tasks.py"
      provides: "Job task definitions"
      contains: "@job_queue.task"
    - path: "src/application/handlers/__init__.py"
      provides: "Handler registration function"
      exports: ["register_all_handlers"]
    - path: "src/application/handlers/user_handlers.py"
      provides: "User event handlers"
      contains: "def on_user_registered"
    - path: "tests/integration/test_event_publishing.py"
      provides: "Integration tests for event publishing"
      min_lines: 60
  key_links:
    - from: "src/adapters/persistence/unit_of_work.py"
      to: "src/application/event_bus.py"
      via: "publish_all after commit"
      pattern: "publish_all\\(events_to_publish\\)"
    - from: "src/adapters/api/app.py"
      to: "src/application/handlers/__init__.py"
      via: "lifespan calls register_all_handlers"
      pattern: "register_all_handlers\\(\\)"
    - from: "src/application/handlers/user_handlers.py"
      to: "src/adapters/jobs/tasks.py"
      via: "handler enqueues job"
      pattern: "defer_async"
---

<objective>
Complete the event publishing infrastructure by wiring the job queue, modifying UoW to publish after commit, and registering handlers at startup.

Purpose: Make events actually flow from domain operations through to handlers. This completes the event-driven architecture foundation. After this plan, UserRegistered events will be published and handled (handler will enqueue a placeholder job for Phase 6 email).

Output:
- Procrastinate job queue app configured for separate database
- UoW publishes events after successful commit
- Handlers registered during FastAPI lifespan
- Worker runs in same process as API
- Integration tests proving the full flow works
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/CHECKPOINTS.md
@.planning/phases/05-domain-event-publishing/05-CONTEXT.md
@.planning/phases/05-domain-event-publishing/05-RESEARCH.md
@.planning/phases/05-domain-event-publishing/05-01-SUMMARY.md

# Key files to modify
@src/adapters/persistence/unit_of_work.py
@src/adapters/api/app.py
@src/application/event_bus.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Procrastinate job queue infrastructure</name>
  <files>
    - src/adapters/jobs/__init__.py
    - src/adapters/jobs/app.py
    - src/adapters/jobs/tasks.py
  </files>
  <action>
1. Create `src/adapters/jobs/__init__.py`:
```python
"""Job queue infrastructure using Procrastinate.

Provides async job processing for side effects like email sending,
external API calls, and other potentially-failing operations.
"""
from .app import job_queue

__all__ = ["job_queue"]
```

2. Create `src/adapters/jobs/app.py` following the pattern from 05-RESEARCH.md:
```python
"""Procrastinate job queue application.

Uses a separate PostgreSQL database from the main app as per
architecture decisions. Connection via JOB_QUEUE_DATABASE_URL.
"""
import os
from procrastinate import App, PsycopgConnector

def create_job_queue() -> App:
    """Create and configure Procrastinate job queue app."""
    conninfo = os.getenv(
        "JOB_QUEUE_DATABASE_URL",
        "postgresql://postgres:postgres@postgres-jobs:5432/jobs"
    )
    return App(
        connector=PsycopgConnector(conninfo=conninfo),
        import_paths=["src.adapters.jobs.tasks"],
    )

# Singleton instance
job_queue = create_job_queue()
```

3. Create `src/adapters/jobs/tasks.py` with placeholder task for email:
```python
"""Job task definitions.

Tasks are decorated with @job_queue.task and executed by workers.
Each task should be idempotent and handle its own error logging.
"""
from src.adapters.jobs.app import job_queue
from src.adapters.logging import get_logger
from procrastinate import RetryStrategy

logger = get_logger(__name__)

# Retry strategy for email tasks (Phase 6 will use this)
email_retry = RetryStrategy(
    max_attempts=5,
    exponential_wait=60,  # 60s, 3600s, 216000s...
    retry_exceptions={ConnectionError, TimeoutError},
)

@job_queue.task(queue="email", retry=email_retry)
async def send_verification_email(user_id: str, email: str, verification_token: str) -> None:
    """Send verification email to newly registered user.

    This is a placeholder - actual email sending implemented in Phase 6.
    """
    logger.info(
        "verification_email_job_executed",
        user_id=user_id,
        # Note: Not logging email address (PII) per CONTEXT.md
    )
    # Phase 6 will implement actual email sending
    pass
```

Note: Use `get_logger` from existing `src/adapters/logging` module.
  </action>
  <verify>
    - `python -c "from src.adapters.jobs import job_queue"` succeeds
    - `python -c "from src.adapters.jobs.tasks import send_verification_email"` succeeds
    - `ruff check src/adapters/jobs/` passes
  </verify>
  <done>
    - Job queue app instance created with PsycopgConnector
    - Placeholder email task defined with retry strategy
    - Logging follows project conventions (no PII)
  </done>
</task>

<task type="auto">
  <name>Task 2: Modify UoW to publish events and wire handlers</name>
  <files>
    - src/adapters/persistence/unit_of_work.py
    - src/application/handlers/__init__.py
    - src/application/handlers/user_handlers.py
  </files>
  <action>
1. Modify `src/adapters/persistence/unit_of_work.py` commit() method:

CRITICAL: Events MUST be published AFTER commit succeeds (see RESEARCH.md Pitfall 1).

Update the commit() method to:
- Capture events before commit clears them: `events_to_publish = list(self._events)`
- After `self.session.commit()` and `self._events.clear()`, add:
  ```python
  # Publish events to handlers AFTER commit succeeds
  from src.application.event_bus import publish_all
  publish_all(events_to_publish)
  ```

The import inside the method avoids circular import issues.

2. Create `src/application/handlers/__init__.py`:
```python
"""Event handler registration.

Handlers are registered at application startup. Each handler
receives domain events and may enqueue jobs for async processing.
"""
from src.application.event_bus import register
from src.domain.events.user_events import UserRegistered, EmailVerified

from .user_handlers import on_user_registered, on_email_verified

def register_all_handlers() -> None:
    """Register all event handlers.

    Called during application startup in FastAPI lifespan.
    """
    # User domain handlers
    register(UserRegistered, on_user_registered)
    register(EmailVerified, on_email_verified)
```

3. Create `src/application/handlers/user_handlers.py`:
```python
"""Handlers for User domain events.

These handlers are called synchronously after UoW commit.
For side effects (email, external calls), they enqueue jobs.
"""
from src.adapters.logging import get_logger
from src.domain.events.user_events import UserRegistered, EmailVerified

logger = get_logger(__name__)


def on_user_registered(event: UserRegistered) -> None:
    """Handle UserRegistered event.

    Enqueues welcome/verification email job.
    Actual email sending is implemented in Phase 6.
    """
    logger.info(
        "user_registered_handler",
        user_id=event.user_id,
        household_id=event.household_id,
        # Note: Not logging email (PII)
    )
    # Phase 6 will add: await send_verification_email.defer_async(...)
    # For now, just log that the handler was called


def on_email_verified(event: EmailVerified) -> None:
    """Handle EmailVerified event.

    Currently a no-op. Future phases may add welcome email, analytics, etc.
    """
    logger.info(
        "email_verified_handler",
        user_id=event.user_id,
    )
```

Note: Handlers are sync for now. Phase 6 will make on_user_registered async
when it needs to call defer_async().
  </action>
  <verify>
    - `python -c "from src.application.handlers import register_all_handlers"` succeeds
    - `ruff check src/application/handlers/` passes
    - `ruff check src/adapters/persistence/unit_of_work.py` passes
  </verify>
  <done>
    - UoW commit() publishes events after successful commit
    - Handler registry function exists
    - User event handlers exist (placeholder implementations)
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire lifespan and add integration tests</name>
  <files>
    - src/adapters/api/app.py
    - tests/integration/test_event_publishing.py
  </files>
  <action>
1. Modify `src/adapters/api/app.py` lifespan to:
   - Import and call `register_all_handlers()` after mappers initialized
   - Import job_queue and start worker in background (using asyncio.create_task)

Update lifespan function:
```python
import asyncio
from src.application.handlers import register_all_handlers
from src.adapters.jobs import job_queue

@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    # ... existing startup code ...

    # Register event handlers
    register_all_handlers()
    logger.info("event_handlers_registered")

    # Start job queue worker in background
    async with job_queue.open_async():
        worker_task = asyncio.create_task(
            job_queue.run_worker_async(install_signal_handlers=False)
        )
        logger.info("job_queue_worker_started")

        yield

        # Shutdown: cancel worker
        worker_task.cancel()
        try:
            await asyncio.wait_for(worker_task, timeout=10)
        except (asyncio.TimeoutError, asyncio.CancelledError):
            pass
        logger.info("job_queue_worker_stopped")

    # ... existing shutdown code ...
```

IMPORTANT: The worker starts inside the `async with job_queue.open_async()` context.
This ensures the connector is open before worker tries to process jobs.

2. Create `tests/integration/test_event_publishing.py`:

Test cases:
- `test_user_registration_publishes_event` - register user, verify handler was called
- `test_commit_publishes_events_to_handlers` - direct UoW test with mock handler
- `test_multiple_handlers_called_for_same_event` - verify all handlers invoked
- `test_handler_exception_does_not_prevent_commit` - (skip if fail-fast behavior is desired)

Use pytest fixtures from existing auth tests for user creation.
Use `event_bus.clear_handlers()` in fixture for test isolation.
Register a test handler that appends to a list to verify invocation.

Example test:
```python
import pytest
from dataclasses import dataclass
from src.application import event_bus
from src.domain.events.user_events import UserRegistered

@pytest.fixture(autouse=True)
def clear_event_handlers():
    yield
    event_bus.clear_handlers()

def test_uow_commit_publishes_events(session_factory):
    """Events collected by UoW are published after commit."""
    received_events = []

    def test_handler(event):
        received_events.append(event)

    event_bus.register(UserRegistered, test_handler)

    # Create UoW, add user, commit
    # ... (use existing patterns from auth tests)

    assert len(received_events) == 1
    assert isinstance(received_events[0], UserRegistered)
```

Note: Job queue worker tests are harder (require DB setup). For now, test event bus
flow only. Job execution can be tested when Phase 6 adds real email task.
  </action>
  <verify>
    - Start the service: `uvicorn src.adapters.api.app:app --host 0.0.0.0 --port 8000 &`
    - Wait for startup, check logs show "event_handlers_registered" and "job_queue_worker_started"
    - `curl -s http://localhost:8000/docs | head -5` returns HTML
    - `pytest tests/integration/test_event_publishing.py -v` passes
    - `pytest tests/ -x -q` all tests pass
  </verify>
  <done>
    - FastAPI lifespan registers handlers and starts worker
    - Integration tests verify event publishing flow
    - All existing tests still pass
    - Service starts and responds to requests
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Service starts with event infrastructure:
   ```bash
   # Start service (ensure postgres-jobs is running first)
   docker compose up -d postgres postgres-jobs
   sleep 5
   # Apply Procrastinate schema to jobs database
   procrastinate --app=src.adapters.jobs.app.job_queue schema --apply
   # Start the service
   uvicorn src.adapters.api.app:app --host 0.0.0.0 --port 8000 &
   sleep 3
   curl -s http://localhost:8000/docs | head -5
   ```

2. Logs show infrastructure started:
   - "event_handlers_registered"
   - "job_queue_worker_started"

3. All tests pass:
   ```bash
   pytest tests/ -x -q
   ```

4. Register a user and verify handler logged:
   ```bash
   curl -X POST http://localhost:8000/auth/register \
     -H "Content-Type: application/json" \
     -d '{"email": "test@example.com", "password": "TestPass123!"}'
   # Check logs for "user_registered_handler"
   ```
</verification>

<success_criteria>
1. Job queue app created with separate database connection
2. UoW commit() publishes events after transaction succeeds
3. Handler registration happens during FastAPI startup
4. Worker runs in same process as API via lifespan
5. Integration tests pass proving event flow works
6. UserRegistered event triggers on_user_registered handler
7. All existing 414+ tests still pass
8. Service starts and responds to smoke test requests
</success_criteria>

<output>
After completion, create `.planning/phases/05-domain-event-publishing/05-02-SUMMARY.md`
</output>
